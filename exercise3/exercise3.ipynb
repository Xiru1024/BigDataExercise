{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6be0b4e-c924-425d-bfad-6ce9c43a7f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17430727"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "pd.set_option('display.max_rows', 20000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Exercise3\").getOrCreate()\n",
    "url = \"https://raw.githubusercontent.com/Xiru1024/BigDataExercise/refs/heads/main/exampleData.csv\"\n",
    "local_path = \"/home/jovyan/BigDataExercise/exercise3/example.csv\"\n",
    "with open(local_path, \"wb\") as f:\n",
    "    f.write(requests.get(url).content)\n",
    "df = spark.read.csv(local_path, header=True, inferSchema=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e66ae4d-4aa5-461d-9548-4ad5a92d2e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"dateTime\" ,outputCol=\"indexedTime\")\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "# indexed.show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faeac6b5-725a-4bcf-82f4-10690bf780e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data = indexed.selectExpr(\"cast(dateTime as string)dateTime\",\n",
    " \"cast(relative_humidity as float) relative_humidity\",\"cast(wind_speed as float) wind_speed\",\n",
    " \"cast(indexedTime as double) indexedTime\",\"cast(air_temperature as float) label\",\"cast(msl as float) msl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "405bfeb9-a8b2-44a7-b751-8ceef998dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols = ['relative_humidity',\n",
    "'wind_speed', 'indexedTime', 'msl'], handleInvalid=\"skip\",outputCol =\n",
    "'features')\n",
    "vectorized_df = vectorAssembler.transform(parsed_data)\n",
    "dataset = vectorized_df.select(\"features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f0c2a9d-b4c2-48a6-b473-e457a7445a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6e511a1-2764-4804-95b1-d6e23fcd3bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(featuresCol=\"features\", maxIter=30, maxDepth = 11)\n",
    "# Train model. This also runs the indexer.\n",
    "model = gbt.fit(trainingData)\n",
    "# Make predictions.\n",
    "prediction = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10a99bc1-b017-43ac-88ef-7c5204811374",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (2091508349.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    testing)\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "# Split the data into training and test sets (30% held out for\n",
    "testing)\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3])\n",
    "# Create a VectorIndexer (do not pre-fit; it will be fit as part of\n",
    "the pipeline)\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\",\n",
    "outputCol=\"indexedFeatures\", handleInvalid=\"skip\")\n",
    "# Train a GBT model.\n",
    "gbt = GBTRegressor(featuresCol=\"indexedFeatures\", maxIter=30,\n",
    "maxDepth=11)\n",
    "# Chain indexer and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "# Train model. This also fits the indexer on the training data.\n",
    "model_gbt = pipeline.fit(trainingData)\n",
    "# Make predictions.\n",
    "preds = model_gbt.transform(testData)\n",
    "# Evaluate the model by computing the R Squared on test data.\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\",\n",
    "predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator.evaluate(preds)\n",
    "print(\"R Squared on test data = %g\" % r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41574516-f5f2-4b83-8b2d-5e83d3ba9d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    " labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae= evaluator.evaluate(preds)\n",
    "print(\"MAE on test data = %g\" % mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2cc363-8dd6-445d-a57b-6c1aa1da75a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    " labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(preds)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b416faa7-a595-441e-9f11-dda05da4a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pd = preds.toPandas()\n",
    "import seaborn\n",
    "from scipy.stats import *\n",
    "seaborn.set(style=\"whitegrid\", font_scale = 1.8)\n",
    "fig, ax = plt.subplots()\n",
    "seaborn.set(color_codes=True)\n",
    "seaborn.set(rc={'figure.figsize':(20, 10)})\n",
    "seaborn.regplot(x=\"label\", y=\"prediction\", fit_reg=False, ax=ax,data\n",
    "=pred_pd,scatter_kws={\"color\": \"b\"});\n",
    "seaborn.regplot(x=\"label\", y=\"prediction\",scatter=False, ax=ax, data\n",
    "=pred_pd, line_kws={\"color\": \"red\"});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ee6f7f4-9116-4a27-b576-19a56d9adcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[2667.0,0.3,190.0...|  1.0|\n",
      "|[2668.0,0.2,220.0...|  0.0|\n",
      "|[2669.0,0.0,190.0...|  0.0|\n",
      "|[2670.0,0.0,190.0...|  0.0|\n",
      "|[2671.0,0.0,220.0...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(6,[0,5],[235846....|  2.0|       1.0|\n",
      "|(6,[0,5],[235851....|  2.0|       1.0|\n",
      "|(6,[0,5],[235857....|  2.0|       1.0|\n",
      "|(6,[0,5],[235858....|  2.0|       1.0|\n",
      "|(6,[0,5],[235859....|  2.0|       1.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test set accuracy = 0.366047\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "# Cast relevant columns to double\n",
    "df_numeric = indexed.withColumn(\"air_temperature\",\n",
    "col(\"air_temperature\").cast(\"double\")) \\\n",
    ".withColumn(\"precipitation\",\n",
    "col(\"precipitation\").cast(\"double\")) \\\n",
    ".withColumn(\"wind_from_direction\",\n",
    "col(\"wind_from_direction\").cast(\"double\")) \\\n",
    ".withColumn(\"relative_humidity\",\n",
    "col(\"relative_humidity\").cast(\"double\")) \\\n",
    ".withColumn(\"wind_speed\",\n",
    "col(\"wind_speed\").cast(\"double\")) \\\n",
    ".withColumn(\"msl\", col(\"msl\").cast(\"double\"))\n",
    "# Bin air_temperature into 3 classes (for classification using MLP)\n",
    "discretizer = QuantileDiscretizer(numBuckets=3,\n",
    "inputCol=\"air_temperature\", outputCol=\"binnedLabel\")\n",
    "binned_df = discretizer.fit(df_numeric).transform(df_numeric)\n",
    "# Rename binnedLabel to 'label' for use as the target\n",
    "binned_df = binned_df.withColumnRenamed(\"binnedLabel\", \"label\")\n",
    "# Prepare features by assembling selected numeric columns\n",
    "vectorAssembler = VectorAssembler(\n",
    "inputCols=['indexedTime', 'precipitation', 'wind_from_direction',\n",
    "'relative_humidity', 'wind_speed', 'msl'],\n",
    "handleInvalid=\"skip\",\n",
    "outputCol='features'\n",
    ")\n",
    "vectorized_df = vectorAssembler.transform(binned_df)\n",
    "# Select only the features and label for training\n",
    "dataset_class = vectorized_df.select(\"features\", \"label\")\n",
    "dataset_class.show(5)\n",
    "# Split the data\n",
    "(trainingData, testData) = dataset_class.randomSplit([0.7, 0.3])\n",
    "# Define the MLP classifier; adjust the input layer size as per your features\n",
    "layers = [6, 12,3]\n",
    "mlp = MultilayerPerceptronClassifier(maxIter=800, layers=layers,\n",
    "blockSize=128, seed=1234)\n",
    "# Train the model\n",
    "model_mlp = mlp.fit(trainingData)\n",
    "# Make predictions\n",
    "predictions_mlp = model_mlp.transform(testData)\n",
    "predictions_mlp.select(\"features\", \"label\", \"prediction\").show(5)\n",
    "# Evaluate using a suitable classification evaluator, e.g., accuracy\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions_mlp)\n",
    "print(\"Test set accuracy = %g\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa954481-6a47-40dc-b65f-438cae1f1e09",
   "metadata": {},
   "source": [
    "200 iterations - 0.36806"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec9d7f4-75d3-48b3-b71d-0419eb8a368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "300 iterations -  0.368668"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae4703c-fe29-498c-9376-b63c60c58620",
   "metadata": {},
   "outputs": [],
   "source": [
    "500 iterations  0.370278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e78598d-6de0-49c5-a5d8-39050ba6365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "block size=64 - 0.362132"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5977236-134e-4ec4-bde7-7ea3fb58c7ee",
   "metadata": {},
   "source": [
    "block size 64 and iteration 200 and add additional layer 24 - 0.366763\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08512a3d-9e90-4ff1-8f93-4166132ace30",
   "metadata": {},
   "outputs": [],
   "source": [
    "base - 0.364516"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
